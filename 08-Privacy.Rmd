# Privacy
Privacy has been the poster child for problems with LLMs for quite some time now, especially when it comes to the privacy of one’s personal information. There are two main obstacles that stand in the way of true privacy in LLMs: Infrastructure and IP protection. According to Daniel Huynh of Mithril Security, requirements for privacy in infrastructure are “the hosting the model in specialized data centers, e.g., in the Cloud, removes the need for complex infrastructure. It becomes pretty easy to feed data to these models and get a result.” As it relates to IP Protection, Huynh believes that “it becomes much more challenging for AI consumers to steal the model's weights (notwithstanding model extraction attacks with a black-box API).” [^1]

## Scaling Laws & Specialized Data Centers Energy Impact
Mithril Security reflects on a trend discussed in a paper by a team of Computer Scientists at Cornell University called “Scaling Laws for Neural Language Model,” which was published in January of 2020. In this paper, the team is reporting their findings on the study of empirical scaling laws for language model performance on cross-entropy loss. The trend that is discussed is that it seems “the loss scales as a power-law with model size, dataset size, and the amount of compute used for training.” [^2] Daniel Huynh takes this scale of power-law and discusses how “given the power requirements of those models, it also makes sense from an ecological perspective to rely on specialized data centers rather than on-premise deployment.” Furthermore, “economies of scale that Cloud providers leverage are more energy efficient than companies employing on premise.”

Referring back to the first source, this visualization shows how data centers have become increasingly energy efficient. Why Huynh emphasizes this energy efficiency in cloud hosting is because according to the team at Cornell, “simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on the model size”. Basically, cloud hosting is the simplest form of being able to handle the large-scale requirements of LLMs currently. 

## Cloud Hosting & Privacy Enhancing Technologies
As it relates back to privacy in LLMs, cloud hosting comes with a cost. According to Mithril Security, “sensitive data becomes potentially exposed to the actors providing those large models and the Cloud Provider.” Unfortunately, given the current processes of these systems, sensitive data being potentially exposed can happen rather easily. “If one actor (either the Cloud or the solution provider) is compromised or malicious, thousands of users' data can be exposed.” Mithril Security is determined to lead the next steps in LLM production through Privacy Enhancing Technologies. Specifically, Mithril Security has a Privacy Enhancing Technology called BlindAI. BlindAI is “an open-source solution to query and deploy AI models while guaranteeing data privacy. The querying of models is done via our easy-to-use Python library. How it works is “data sent by users to the AI model is kept confidential at all times by hardware-enforced Trusted Execution Environments.” There are two main uses for BlindAI: BlindAI API and BlindAI Core. BlindAI API is “using BlindAI to query popular AI models hosted by Mithril Security.” BlindAI Core is “using BlindAI's underlying technology to host your own BlindAI server instance to securely deploy your own models.” [^3] The visual below shows BlindAI in practice. 

## AutoML
Another current process for privacy in LLMs is AutoML. “AutoML refers to the automated process of end-to-end development of machine learning models. It involves automating data preprocessing, feature selection, model selection, and hyperparameter tuning. It allows users with varying levels of expertise to develop machine-learning models with high efficiency and minimal manual intervention.” There are several reasons to employ AutoML. First and foremost, “organizations can maintain full control of their data.” Obviously, keeping sensitive information in-house limits the transferring of information, thus reducing the chance it is exposed. According to Manuel Herranz, the CEO of Pangeanic, “this approach ensures data privacy regulations compliance, a crucial aspect in sectors like healthcare, finance, and education, where stringent data privacy rules apply.” Furthermore, self-training LLMs like AutoML “reduces the dependency on external tech giants, providing more autonomy to organizations. [^4] BlindAI and AutoML are two different ways to increase privacy in the production of LLMs.

[^1]: https://blog.mithrilsecurity.io/privacy-for-adoption-of-big-models/
[^2]: https://arxiv.org/abs/2001.08361?ref=blog.mithrilsecurity.io
[^3]: https://github.com/mithril-security/blindai?ref=blog.mithrilsecurity.io
[^4]:https://blog.pangeanic.com/automl-and-llm-self-training-maintaining-data-privacy-in-the-ai-revolution 




