# Bias
Humans are referred to by many in the data field as the most advanced form of Artificial Intelligence (AI) and Large Language Models (LLM). Yes, as these two big data concepts have become more commonplace in society today; data analysts, scientists, professionals, and students alike have all relied on the use of AI and LLMs at some point for a specific purpose. However, what makes people more advanced than AI and LLMs is intuition and context. These are two soft, intangible things LLMs cannot possess.

## The Subtle Bias in Training Data
Anees Merchant, Head of Global Growth and Client Success - Digital, Applied AI, and Research at Core5 Intelligence expands on this notion of bias in LLMs in a post on his personal blog titled “Large Language Models and Bias: An Unresolved Issue”. Within this blog post, Merchant discusses the fact that bias “can be as subtle as a model associating certain occupations with a specific gender or as blatant as a model generating offensive or harmful content.” In simple terms, LLMs when prompted will associate Doctor to being a male occupation and Nurse to being a female occupation. How does this happen? It has everything to do with the data that these models are trained on. According to Merchant, “if the training data contains biased information, the model will inevitably learn and reproduce these biases.” The problem with this is that it can perpetuate harmful stereotypes and discrimination. For example, if an LLM is used as a hiring tool in an engineering field, it may put female applicants at a disadvantage [^1].

## MIT Study
In a study performed by MIT on the popular Large Language Model ChatGPT, when prompted about bias, the response is, “Yes, language models can have biases, because the training data reflects the biases present in society from which that data was collected. For example, gender and racial biases are prevalent in many real-world datasets, and if a language model is trained on that, it can perpetuate and amplify these biases in its predictions”. There within itself is the primary issue. Although ChatGPT has the awareness to understand its biased tendencies, due to the way it is programmed, it cannot produce answers that mitigate bias. According to the researchers at MIT, “current language models suffer from issues with fairness, computational resources, and privacy”. The research conducted also returned the conclusion that “language models have similar properties. A language model without explicit logic learning makes plenty of biased reasoning, but adding logic learning can significantly mitigate such behavior” [^2].

## Addressing The Issue
The question now becomes how do we address the issue? Referencing the first source, while there is no clear answer, Merchant suggests that it is a combination of technical and non-technical approaches and the involvement of various stakeholders. They will then attempt to develop methods during the model training process that aims to reduce the influence of biased patterns in training data. It is important from a non-technical perspective to have diverse teams working on the development and deployment of LLMs. There is, however, an education piece addressing the issue as well. According to Merchant, transparency and interpretability are crucial. Understanding and explaining how a model makes decisions can help identify and mitigate bias.

## Hallucinations
Another layer to the biases within Large Language Models is hallucinations. A hallucination refers to the phenomenon where the model generates text that is incorrect, nonsensical, or not real. This is because LLMs are not databases or search engines; therefore they cannot cite what their response is based on. A very basic example of hallucinations in LLMs is building a two-letter bigrams Markov model from text. As the model begins to pull out the two-letter bigrams from the text, the probabilities change because of the statistical patterns. Eventually, when prompted by a letter, the LLM will return a word it invented that does not exist. Hallucinations are caused by “limited contextual understanding since the model is obligated to transform the prompt and the training data into an abstraction” [^3]. 

[^1]:https://www.aneesmerchant.com/personal-musings/large-language-models-and-bias-an-unresolved-issue#:~:text=The%20Nature%20of%20Bias%20in,to%20generate%20human%2Dlike%20text
[^2]:https://news.mit.edu/2023/large-language-models-are-biased-can-logic-help-save-them-0303
[^3]:https://machinelearningmastery.com/a-gentle-introduction-to-hallucinations-in-large-language-models/




